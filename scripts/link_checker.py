#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™é“¾æ¥æ£€æµ‹å·¥å…·
æ£€æŸ¥webstack.ymlä¸­æ‰€æœ‰ç½‘ç«™é“¾æ¥çš„å¯ç”¨æ€§
"""

import yaml
import requests
import time
import argparse
import sys
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urlparse
from datetime import datetime
import json

class LinkChecker:
    def __init__(self, webstack_file):
        self.webstack_file = webstack_file
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # çŠ¶æ€ç»Ÿè®¡
        self.stats = {
            'total': 0,
            'success': 0,
            'failed': 0,
            'timeout': 0,
            'redirect': 0,
            'ssl_error': 0,
            'dns_error': 0,
            'unknown_error': 0
        }
        
        # å¤±è´¥çš„é“¾æ¥è¯¦æƒ…
        self.failed_links = []
        
    def load_webstack_data(self):
        """åŠ è½½webstack.ymlæ•°æ®"""
        try:
            with open(self.webstack_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.webstack_file}")
            sys.exit(1)
        except yaml.YAMLError as e:
            print(f"âŒ YAMLè§£æé”™è¯¯: {e}")
            sys.exit(1)
    
    def extract_all_links(self, data):
        """æå–æ‰€æœ‰ç½‘ç«™é“¾æ¥"""
        links = []
        
        def extract_from_category(items, category_name=""):
            for item in items:
                if 'taxonomy' in item:
                    # è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»
                    category_title = item.get('taxonomy', 'Unknown Category')
                    if 'list' in item:
                        # å¤„ç†listä¸‹çš„å­åˆ†ç±»
                        for sub_item in item['list']:
                            if 'term' in sub_item:
                                sub_category = sub_item.get('term', 'Unknown Subcategory')
                                full_category = f"{category_title} > {sub_category}"
                                if 'links' in sub_item:
                                    extract_from_category(sub_item['links'], full_category)
                            elif 'links' in sub_item:
                                extract_from_category(sub_item['links'], category_title)
                elif 'term' in item:
                    # è¿™æ˜¯ä¸€ä¸ªå­åˆ†ç±»
                    sub_category = item.get('term', 'Unknown Subcategory')
                    if 'links' in item:
                        extract_from_category(item['links'], f"{category_name} > {sub_category}")
                else:
                    # è¿™æ˜¯ä¸€ä¸ªé“¾æ¥
                    url = item.get('url', '').strip()
                    title = item.get('title', 'Unknown')
                    description = item.get('description', '')
                    logo = item.get('logo', '')
                    
                    if url:
                        links.append({
                            'url': url,
                            'title': title,
                            'description': description,
                            'logo': logo,
                            'category': category_name
                        })
        
        if isinstance(data, list):
            extract_from_category(data)
        elif isinstance(data, dict) and 'webstack' in data:
            extract_from_category(data['webstack'])
        
        return links
    
    def normalize_url(self, url):
        """æ ‡å‡†åŒ–URLæ ¼å¼"""
        if not url:
            return None
        
        url = url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        return url
    
    def is_china_domain(self, url):
        """æ£€æµ‹æ˜¯å¦ä¸ºä¸­å›½åŸŸå"""
        if not url:
            return False
        
        try:
            domain = urlparse(url).netloc.lower()
            china_suffixes = ['.cn', '.com.cn', '.net.cn', '.org.cn', '.gov.cn', '.edu.cn']
            return any(domain.endswith(suffix) for suffix in china_suffixes)
        except:
            return False
    
    def check_single_link(self, link_info, timeout=10):
        """æ£€æŸ¥å•ä¸ªé“¾æ¥çš„å¯ç”¨æ€§"""
        url = self.normalize_url(link_info['url'])
        if not url:
            return {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status': 'invalid_url',
                'status_code': None,
                'response_time': None,
                'final_url': None,
                'error': 'Invalid URL format'
            }
        
        # ä¸­å›½ç½‘ç«™ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
        if self.is_china_domain(url):
            timeout = 20
        
        start_time = time.time()
        
        try:
            response = self.session.get(
                url, 
                timeout=timeout, 
                allow_redirects=True,
                verify=False  # å¿½ç•¥SSLè¯ä¹¦éªŒè¯
            )
            
            response_time = time.time() - start_time
            
            result = {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status_code': response.status_code,
                'response_time': round(response_time, 2),
                'final_url': response.url if response.url != url else None,
                'error': None
            }
            
            if response.status_code == 200:
                result['status'] = 'success'
                self.stats['success'] += 1
            elif 300 <= response.status_code < 400:
                result['status'] = 'redirect'
                self.stats['redirect'] += 1
            else:
                result['status'] = 'http_error'
                result['error'] = f'HTTP {response.status_code}'
                self.stats['failed'] += 1
                self.failed_links.append(result)
            
            return result
            
        except requests.exceptions.Timeout:
            result = {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status': 'timeout',
                'status_code': None,
                'response_time': timeout,
                'final_url': None,
                'error': f'Timeout after {timeout}s'
            }
            self.stats['timeout'] += 1
            self.failed_links.append(result)
            return result
            
        except requests.exceptions.SSLError:
            result = {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status': 'ssl_error',
                'status_code': None,
                'response_time': time.time() - start_time,
                'final_url': None,
                'error': 'SSL Certificate Error'
            }
            self.stats['ssl_error'] += 1
            self.failed_links.append(result)
            return result
            
        except requests.exceptions.ConnectionError as e:
            error_msg = str(e)
            if 'Name or service not known' in error_msg or 'getaddrinfo failed' in error_msg:
                status = 'dns_error'
                self.stats['dns_error'] += 1
            else:
                status = 'connection_error'
                self.stats['failed'] += 1
            
            result = {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status': status,
                'status_code': None,
                'response_time': time.time() - start_time,
                'final_url': None,
                'error': 'Connection failed'
            }
            self.failed_links.append(result)
            return result
            
        except Exception as e:
            result = {
                'url': link_info['url'],
                'title': link_info['title'],
                'category': link_info['category'],
                'status': 'unknown_error',
                'status_code': None,
                'response_time': time.time() - start_time,
                'final_url': None,
                'error': str(e)
            }
            self.stats['unknown_error'] += 1
            self.failed_links.append(result)
            return result
    
    def check_links_batch(self, links, max_workers=3, verbose=True):
        """æ‰¹é‡æ£€æŸ¥é“¾æ¥"""
        if verbose:
            print(f"ğŸ” å¼€å§‹æ£€æŸ¥ {len(links)} ä¸ªé“¾æ¥...")
            print(f"âš™ï¸  ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œçº¿ç¨‹\n")
        
        self.stats['total'] = len(links)
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_link = {
                executor.submit(self.check_single_link, link): link 
                for link in links
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(as_completed(future_to_link), 1):
                link = future_to_link[future]
                
                try:
                    result = future.result()
                    results.append(result)
                    
                    if verbose:
                        status_emoji = self.get_status_emoji(result['status'])
                        time_info = f" ({result['response_time']}s)" if result['response_time'] else ""
                        print(f"  [{i:3d}/{len(links)}] {status_emoji} {result['title'][:40]}{time_info}")
                    
                except Exception as e:
                    if verbose:
                        print(f"  [{i:3d}/{len(links)}] âŒ æ£€æŸ¥å¤±è´¥: {e}")
                    self.stats['unknown_error'] += 1
                
                # æ·»åŠ å°å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(0.1)
        
        return results
    
    def get_status_emoji(self, status):
        """æ ¹æ®çŠ¶æ€è¿”å›å¯¹åº”çš„è¡¨æƒ…ç¬¦å·"""
        emoji_map = {
            'success': 'âœ…',
            'redirect': 'ğŸ”„',
            'timeout': 'â°',
            'ssl_error': 'ğŸ”’',
            'dns_error': 'ğŸŒ',
            'http_error': 'âŒ',
            'connection_error': 'ğŸ”Œ',
            'invalid_url': 'ğŸš«',
            'unknown_error': 'â“'
        }
        return emoji_map.get(status, 'â“')
    
    def generate_report(self, results, format='markdown'):
        """ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        iso_timestamp = datetime.now().isoformat()
        
        if format == 'markdown':
            return self.generate_markdown_report(results, timestamp, iso_timestamp)
        elif format == 'json':
            return self.generate_json_report(results, timestamp, iso_timestamp)
        else:
            return self.generate_text_report(results, timestamp)
    
    def generate_markdown_report(self, results, timestamp, iso_timestamp):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report = [
            "# ç½‘ç«™é“¾æ¥æ£€æµ‹æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {timestamp}",
            f"**ISOæ—¶é—´**: {iso_timestamp}",
            "",
            "## ğŸ“Š æ£€æµ‹ç»Ÿè®¡",
            "",
            f"- **æ€»é“¾æ¥æ•°**: {self.stats['total']}",
            f"- **æˆåŠŸè®¿é—®**: {self.stats['success']} ({self.stats['success']/self.stats['total']*100:.1f}%)",
            f"- **é‡å®šå‘**: {self.stats['redirect']}",
            f"- **è¶…æ—¶**: {self.stats['timeout']}",
            f"- **SSLé”™è¯¯**: {self.stats['ssl_error']}",
            f"- **DNSé”™è¯¯**: {self.stats['dns_error']}",
            f"- **HTTPé”™è¯¯**: {self.stats['failed']}",
            f"- **å…¶ä»–é”™è¯¯**: {self.stats['unknown_error']}",
            "",
            f"**æ€»æˆåŠŸç‡**: {(self.stats['success'] + self.stats['redirect'])/self.stats['total']*100:.1f}%",
            ""
        ]
        
        if self.failed_links:
            report.extend([
                "## âŒ å¤±è´¥é“¾æ¥è¯¦æƒ…",
                "",
                "| ç½‘ç«™ | åˆ†ç±» | URL | é”™è¯¯ç±»å‹ | è¯¦ç»†ä¿¡æ¯ |",
                "|------|------|-----|----------|----------|"
            ])
            
            for link in self.failed_links:
                error_info = link.get('error', 'æœªçŸ¥é”™è¯¯')
                status_code = f" (HTTP {link['status_code']})" if link['status_code'] else ""
                report.append(
                    f"| {link['title']} | {link['category']} | {link['url']} | {link['status']}{status_code} | {error_info} |"
                )
            
            report.append("")
        
        # æŒ‰åˆ†ç±»æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
        report.extend([
            "## ğŸ“‹ è¯¦ç»†ç»“æœ",
            ""
        ])
        
        # æŒ‰åˆ†ç±»åˆ†ç»„
        by_category = {}
        for result in results:
            category = result['category'] or 'æœªåˆ†ç±»'
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(result)
        
        for category, links in by_category.items():
            report.extend([
                f"### {category}",
                ""
            ])
            
            for link in links:
                status_emoji = self.get_status_emoji(link['status'])
                time_info = f" ({link['response_time']}s)" if link['response_time'] else ""
                
                if link['final_url'] and link['final_url'] != link['url']:
                    redirect_info = f" â†’ [{link['final_url']}]({link['final_url']})"
                else:
                    redirect_info = ""
                
                report.append(f"- {status_emoji} **{link['title']}** - [{link['url']}]({link['url']}){time_info}{redirect_info}")
                
                if link['error']:
                    report.append(f"  - âŒ {link['error']}")
            
            report.append("")
        
        return "\n".join(report)
    
    def generate_json_report(self, results, timestamp, iso_timestamp):
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        report_data = {
            'metadata': {
                'generated_at': timestamp,
                'iso_timestamp': iso_timestamp,
                'total_links': self.stats['total'],
                'success_rate': round((self.stats['success'] + self.stats['redirect'])/self.stats['total']*100, 1)
            },
            'statistics': self.stats,
            'results': results,
            'failed_links': self.failed_links
        }
        
        return json.dumps(report_data, ensure_ascii=False, indent=2)
    
    def generate_text_report(self, results, timestamp):
        """ç”Ÿæˆçº¯æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        report = [
            "=" * 60,
            "ç½‘ç«™é“¾æ¥æ£€æµ‹æŠ¥å‘Š",
            "=" * 60,
            f"ç”Ÿæˆæ—¶é—´: {timestamp}",
            "",
            "ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:",
            f"  æ€»é“¾æ¥æ•°: {self.stats['total']}",
            f"  æˆåŠŸè®¿é—®: {self.stats['success']} ({self.stats['success']/self.stats['total']*100:.1f}%)",
            f"  é‡å®šå‘: {self.stats['redirect']}",
            f"  å¤±è´¥é“¾æ¥: {len(self.failed_links)}",
            f"  æ€»æˆåŠŸç‡: {(self.stats['success'] + self.stats['redirect'])/self.stats['total']*100:.1f}%",
            ""
        ]
        
        if self.failed_links:
            report.extend([
                "âŒ å¤±è´¥é“¾æ¥:",
                "-" * 40
            ])
            
            for link in self.failed_links:
                report.append(f"â€¢ {link['title']} ({link['url']})")
                report.append(f"  é”™è¯¯: {link['status']} - {link.get('error', 'æœªçŸ¥é”™è¯¯')}")
                report.append("")
        
        return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(description='ç½‘ç«™é“¾æ¥æ£€æµ‹å·¥å…·')
    parser.add_argument('--webstack', default='../data/webstack.yml',
                       help='webstack.ymlæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--threads', type=int, default=3,
                       help='å¹¶è¡Œçº¿ç¨‹æ•° (é»˜è®¤3)')
    parser.add_argument('--format', choices=['markdown', 'json', 'text'], default='markdown',
                       help='æŠ¥å‘Šæ ¼å¼ (é»˜è®¤markdown)')
    parser.add_argument('--output', '-o', 
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: link_check_report.md/json/txt)')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œåªæ˜¾ç¤ºæœ€ç»ˆç»“æœ')
    parser.add_argument('--failed-only', action='store_true',
                       help='åªæ˜¾ç¤ºå¤±è´¥çš„é“¾æ¥')
    
    args = parser.parse_args()
    
    # ç¡®å®šwebstackæ–‡ä»¶è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    webstack_file = os.path.join(script_dir, args.webstack)
    
    checker = LinkChecker(webstack_file)
    
    if not args.quiet:
        print("ğŸ”— ç½‘ç«™é“¾æ¥æ£€æµ‹å·¥å…· v1.0\n")
    
    # åŠ è½½æ•°æ®å¹¶æå–é“¾æ¥
    data = checker.load_webstack_data()
    links = checker.extract_all_links(data)
    
    if not links:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥")
        sys.exit(1)
    
    # æ‰§è¡Œæ£€æµ‹
    results = checker.check_links_batch(links, max_workers=args.threads, verbose=not args.quiet)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = checker.generate_report(results, format=args.format)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶
    if args.output:
        output_file = args.output
    else:
        results_dir = os.path.join(script_dir, 'results')
        os.makedirs(results_dir, exist_ok=True)
        file_extensions = {'markdown': '.md', 'json': '.json', 'text': '.txt'}
        output_file = os.path.join(results_dir, f'link_check_report{file_extensions[args.format]}')
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # æ˜¾ç¤ºç»“æœ
    if not args.quiet:
        print(f"\nğŸ“Š æ£€æµ‹å®Œæˆ!")
        print(f"  æ€»é“¾æ¥æ•°: {checker.stats['total']}")
        print(f"  æˆåŠŸè®¿é—®: {checker.stats['success']} ({checker.stats['success']/checker.stats['total']*100:.1f}%)")
        print(f"  é‡å®šå‘: {checker.stats['redirect']}")
        print(f"  å¤±è´¥é“¾æ¥: {len(checker.failed_links)}")
        print(f"  æ€»æˆåŠŸç‡: {(checker.stats['success'] + checker.stats['redirect'])/checker.stats['total']*100:.1f}%")
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    # åªæ˜¾ç¤ºå¤±è´¥é“¾æ¥
    if args.failed_only and checker.failed_links:
        print(f"\nâŒ å¤±è´¥é“¾æ¥ ({len(checker.failed_links)}ä¸ª):")
        for link in checker.failed_links:
            print(f"  â€¢ {link['title']} - {link['url']}")
            print(f"    é”™è¯¯: {link['status']} - {link.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # è®¾ç½®é€€å‡ºç 
    sys.exit(0 if not checker.failed_links else 1)


if __name__ == "__main__":
    main()
